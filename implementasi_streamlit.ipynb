{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "##########################################################################\n",
    "#libraries & packages\n",
    "import streamlit as st\n",
    "import PyPDF2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import os\n",
    "##########################################################################\n",
    "stopword_in =  set(stopwords.words('indonesian'))\n",
    "stopword_en =  set(stopwords.words('english'))\n",
    "stopword_both = stopword_in.union(stopword_en)\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer_in = factory.create_stemmer()\n",
    "stemmer_en = PorterStemmer()\n",
    "##########################################################################\n",
    "# sidebar\n",
    "st.sidebar.image(\n",
    "    \"https://blog.text-summarize.com/wp-content/uploads/2020/07/differences-between-extractive-and-abstractive-summary-900x506.jpeg\",\n",
    "    width = 300\n",
    ")\n",
    "st.sidebar.title(\"Hi there, Welcome ðŸ‘‹\")\n",
    "st.sidebar.caption(\"\"\"\n",
    "            Want to know the main content quickly, but lazy to read a long text? Summarizing is the answer! \n",
    "            Summarize your text here quickly, set the number of sentences in the summary yourself. \n",
    "            We'll be happy to help you. Hope you enjoy your time here ðŸ˜„.\n",
    "            \"\"\")\n",
    "page = st.sidebar.selectbox(\"Menu\",(\"Summarize Direct Text\",\"Summarize Text Files\"))\n",
    "st.sidebar.caption(\"Creator : Annida Nur Islami [(LinkedIn)](https://www.linkedin.com/in/annida-nur-islami-a23694214/)\")\n",
    "##########################################################################\n",
    "#page1\n",
    "if page == \"Summarize Direct Text\":\n",
    "    st.title(f\"{page} Menu\")\n",
    "    text = st.text_area('Write down the text here : ',height=200)\n",
    "    compress = st.slider('Compression (%) : ', 0, 100, 25)\n",
    "    ok = st.button(\"Summarize\")\n",
    "    \n",
    "    if ok :\n",
    "        # split sentence\n",
    "        text_str = text.replace('\\n', '')\n",
    "        sentences = re.split('\\. |\\.',text_str)\n",
    "        # hapus tanda baca, angka, dan karakter khusus\n",
    "        clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "        # membuat huruf kecil\n",
    "        clean_sentences = [s.lower() for s in clean_sentences]\n",
    "        \n",
    "        #memfilter kata/token penting (seluruh kata kecuali yang termasuk stopword)\n",
    "        tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "        tokenized = [tokenizer.tokenize(s.lower()) for s in clean_sentences]\n",
    "        important_token = []\n",
    "        for sent in tokenized:\n",
    "            filtered = [s for s in sent if s not in stopword_both]\n",
    "            important_token.append(filtered)\n",
    "            \n",
    "        #menggabungkan kata di list yang telah terfilter menjadi sebuah kalimat\n",
    "        sw_removed = [' '.join(t) for t in important_token]\n",
    "        \n",
    "        #mengubah kata menjadi kata dasarnya\n",
    "        stemmed_sent = [stemmer_in.stem(sent) for sent in sw_removed]\n",
    "        stemmed_sent = [stemmer_en.stem(sent) for sent in sw_removed]\n",
    "        #ektraksi fitur (tfidf)\n",
    "        vec = TfidfVectorizer(lowercase=True)\n",
    "        document = vec.fit_transform(stemmed_sent)\n",
    "        document = document.toarray()\n",
    "        \n",
    "        # n adalah variabel integer panjang hasil ringkasan\n",
    "        n = int((compress/100)*len(sentences))\n",
    "        #result merupakan variabel yang menyimpan bobot setiap kalimat\n",
    "        #seberapa penting kalimat tersebut terhadap keseluruhan teks\n",
    "        result = np.sum(document, axis=1)\n",
    "        #diurutkan\n",
    "        result = sorted(result)\n",
    "        #diambil index\n",
    "        top_n = np.argsort(result)[-n:]\n",
    "        summ_index = sorted(top_n)\n",
    "        \n",
    "        result = []\n",
    "        for i in summ_index:\n",
    "            x = sentences[i] + '. \\n'\n",
    "            result.append(x)\n",
    "        result = \"\".join(result)\n",
    "    \n",
    "        st.header(\"Summary Result\")\n",
    "        st.caption(f\"({len(sentences)} Sentences âž¡ {n} Sentences)\")\n",
    "        st.write(result)\n",
    "##########################################################################\n",
    "#page2  \n",
    "else :\n",
    "    st.title(f\"{page} Menu\")\n",
    "    upload_file = st.file_uploader(\"Upload Your Text File\", type = ['txt'])\n",
    "    compress = st.slider('Compression (%) : ', 0, 100, 25)\n",
    "    ok = st.button(\"Summarize\")\n",
    "    \n",
    "    if ok:\n",
    "        if upload_file:\n",
    "            file_name = upload_file.name\n",
    "            file_name = file_name.split(\".\",1)\n",
    "            file_extension = file_name[1]\n",
    "            \n",
    "            sentences = []\n",
    "            for line in upload_file:\n",
    "                line = line.decode()\n",
    "                sentences.append(line)\n",
    "            text = ' '.join(map(str, sentences))\n",
    "            st.subheader(\"Text of The File ðŸ“œ\")\n",
    "            with st.expander(upload_file.name, expanded=False):\n",
    "                st.write(text)\n",
    "\n",
    "            # split sentence\n",
    "            text_str = text.replace('\\n', '')\n",
    "            sentences = re.split('\\. |\\.',text_str)\n",
    "            # hapus tanda baca, angka, dan karakter khusus\n",
    "            clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
    "            # membuat huruf kecil\n",
    "            clean_sentences = [s.lower() for s in clean_sentences]\n",
    "\n",
    "            #memfilter kata/token penting (seluruh kata kecuali yang termasuk stopword)\n",
    "            tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "            tokenized = [tokenizer.tokenize(s.lower()) for s in clean_sentences]\n",
    "            important_token = []\n",
    "            for sent in tokenized:\n",
    "                filtered = [s for s in sent if s not in stopword_both]\n",
    "                important_token.append(filtered)\n",
    "\n",
    "            #menggabungkan kata di list yang telah terfilter menjadi sebuah kalimat\n",
    "            sw_removed = [' '.join(t) for t in important_token]\n",
    "\n",
    "            #mengubah kata menjadi kata dasarnya\n",
    "            stemmed_sent = [stemmer_in.stem(sent) for sent in sw_removed]\n",
    "            stemmed_sent = [stemmer_en.stem(sent) for sent in sw_removed]\n",
    "            #ektraksi fitur (tfidf)\n",
    "            vec = TfidfVectorizer(lowercase=True)\n",
    "            document = vec.fit_transform(stemmed_sent)\n",
    "            document = document.toarray()\n",
    "\n",
    "            # n adalah variabel integer panjang hasil ringkasan\n",
    "            n = int((compress/100)*len(sentences))\n",
    "            #result merupakan variabel yang menyimpan bobot setiap kalimat\n",
    "            #seberapa penting kalimat tersebut terhadap keseluruhan teks\n",
    "            result = np.sum(document, axis=1)\n",
    "            #diurutkan\n",
    "            result = sorted(result)\n",
    "            #diambil index\n",
    "            top_n = np.argsort(result)[-n:]\n",
    "            summ_index = sorted(top_n)\n",
    "\n",
    "            result = []\n",
    "            for i in summ_index:\n",
    "                x = sentences[i] + '. \\n'\n",
    "                result.append(x)\n",
    "            result = \"\".join(result)\n",
    "            st.header(\"Summary Result\")\n",
    "            st.caption(f\"({len(sentences)} Sentences âž¡ {n} Sentences)\")\n",
    "            st.write(result)\n",
    "            \n",
    "            st.download_button(\n",
    "                'Download Result', \n",
    "                result, \n",
    "                \"result.txt\",\n",
    "                \"text/plain\",\n",
    "                key='download-text'\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
